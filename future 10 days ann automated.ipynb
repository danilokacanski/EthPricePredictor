{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-29T19:00:04.634537Z",
     "start_time": "2024-09-29T19:00:04.623509Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras._tf_keras.keras.models import Model\n",
    "from keras._tf_keras.keras.layers import Dense, Input, Dropout, BatchNormalization, LeakyReLU\n",
    "from keras._tf_keras.keras.regularizers import l2"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:00:07.576480Z",
     "start_time": "2024-09-29T19:00:07.572436Z"
    }
   },
   "cell_type": "code",
   "source": "crypto = ['BTC-USD','BNB-USD', 'ETH-USD','SOL-USD']",
   "id": "c07a6a2f1e6c436c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T18:36:03.060785Z",
     "start_time": "2024-09-29T18:36:02.466878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c = crypto[1]\n",
    "df0 = yf.download(c, period='max')\n",
    "df0.tail()"
   ],
   "id": "79204dc14089c9d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2024-09-25  606.523071  607.762939  584.858826  587.352539  587.352539   \n",
       "2024-09-26  587.351379  608.854187  581.228333  596.776917  596.776917   \n",
       "2024-09-27  596.776917  614.473511  596.500977  607.867004  607.867004   \n",
       "2024-09-28  607.867004  617.332397  595.207764  601.567200  601.567200   \n",
       "2024-09-29  601.451599  602.541443  593.228027  597.609314  597.609314   \n",
       "\n",
       "                Volume  \n",
       "Date                    \n",
       "2024-09-25  1744393946  \n",
       "2024-09-26  2081857126  \n",
       "2024-09-27  2144197609  \n",
       "2024-09-28  1858159482  \n",
       "2024-09-29  1599390976  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-09-25</th>\n",
       "      <td>606.523071</td>\n",
       "      <td>607.762939</td>\n",
       "      <td>584.858826</td>\n",
       "      <td>587.352539</td>\n",
       "      <td>587.352539</td>\n",
       "      <td>1744393946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-26</th>\n",
       "      <td>587.351379</td>\n",
       "      <td>608.854187</td>\n",
       "      <td>581.228333</td>\n",
       "      <td>596.776917</td>\n",
       "      <td>596.776917</td>\n",
       "      <td>2081857126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-27</th>\n",
       "      <td>596.776917</td>\n",
       "      <td>614.473511</td>\n",
       "      <td>596.500977</td>\n",
       "      <td>607.867004</td>\n",
       "      <td>607.867004</td>\n",
       "      <td>2144197609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-28</th>\n",
       "      <td>607.867004</td>\n",
       "      <td>617.332397</td>\n",
       "      <td>595.207764</td>\n",
       "      <td>601.567200</td>\n",
       "      <td>601.567200</td>\n",
       "      <td>1858159482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-29</th>\n",
       "      <td>601.451599</td>\n",
       "      <td>602.541443</td>\n",
       "      <td>593.228027</td>\n",
       "      <td>597.609314</td>\n",
       "      <td>597.609314</td>\n",
       "      <td>1599390976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:00:11.597070Z",
     "start_time": "2024-09-29T19:00:11.580283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_model(units=128, optimizer='adam', activation='relu', dropout_rate=0.3, regularization_rate=0.01):\n",
    "    i = Input(shape=(Tx,))\n",
    "    \n",
    "    # First dense layer with LeakyReLU and batch normalization\n",
    "    x = Dense(units, kernel_regularizer=l2(regularization_rate))(i)\n",
    "    x = LeakyReLU(negative_slope=0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Second dense layer with LeakyReLU and batch normalization\n",
    "    x = Dense(units * 2, kernel_regularizer=l2(regularization_rate))(x)\n",
    "    x = LeakyReLU(negative_slope=0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Third dense layer with ReLU and batch normalization\n",
    "    x = Dense(units * 2, activation=activation, kernel_regularizer=l2(regularization_rate))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate * 0.5)(x)  # Reduced dropout for later layers\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(Ty)(x)\n",
    "    \n",
    "    model = Model(inputs=i, outputs=output)\n",
    "    model.compile(loss='mape', optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ],
   "id": "f4582d179738fb97",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:00:14.078035Z",
     "start_time": "2024-09-29T19:00:14.071424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = {\n",
    "    'model__units': [64, 128],  # Increase the range of units\n",
    "    'model__activation': ['relu', 'tanh', 'elu'],  # Add LeakyReLU and ELU\n",
    "    'model__optimizer': ['adam', 'rmsprop', 'nadam'],  # Include Nadam and AdamW\n",
    "    'model__dropout_rate': [0.1, 0.2],  # Wider range of dropout rates\n",
    "    'model__regularization_rate': [ 0.001, 0.01],  # Explore finer regularization rates\n",
    "    'epochs': [50, 100],  # Increase maximum epochs for deeper models\n",
    "}\n"
   ],
   "id": "244ab372cbda46ec",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:00:16.948294Z",
     "start_time": "2024-09-29T19:00:16.942729Z"
    }
   },
   "cell_type": "code",
   "source": "model = KerasRegressor(model=build_model, verbose=2)",
   "id": "92a360536da7bcf4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T19:41:18.375619Z",
     "start_time": "2024-09-29T19:00:32.938627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = 0\n",
    "future_day_array = []\n",
    "for c in crypto:\n",
    "    df0 = yf.download(c, period='max')\n",
    "    df0 = df0[-1500:]\n",
    "    df = df0[['Close']].copy()\n",
    "    df['LogClose'] = np.log(df['Close'])\n",
    "    df['DiffLogClose'] = df['LogClose'].diff()\n",
    "    train = df\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train[['DiffLogClose']])\n",
    "    \n",
    "    train_idx = df.index <= train.index[-1]\n",
    "    df.loc[train_idx, 'ScaledLogReturn'] = train_scaled.flatten()\n",
    "    \n",
    "    series = df['ScaledLogReturn'].dropna().to_numpy()\n",
    "\n",
    "    Tx = 50\n",
    "    Ty = 10\n",
    "    X = []\n",
    "    Y = []\n",
    "    for t in range(len(series) - Tx - Ty + 1):\n",
    "      x = series[t:t+Tx]\n",
    "      X.append(x)\n",
    "      y = series[t+Tx:t+Tx+Ty]\n",
    "      Y.append(y)\n",
    "    X = np.array(X).reshape(-1, Tx)\n",
    "    Y = np.array(Y).reshape(-1, Ty)\n",
    "    \n",
    "    Xtrain_m, Ytrain_m = X, Y\n",
    "    \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2, verbose=2)\n",
    "    grid_result = grid.fit(Xtrain_m, Ytrain_m)\n",
    "    best_model = grid_result.best_estimator_\n",
    "        \n",
    "    future_day_scaled_log_diff = best_model.predict(Xtrain_m[-10:])\n",
    "    future_day_scaled_log_diff = future_day_scaled_log_diff[:,0]\n",
    "    future_day_log_diff = scaler.inverse_transform(future_day_scaled_log_diff.reshape(-1,1)).flatten()\n",
    "    last_train = train.iloc[-1]['LogClose']\n",
    "    future_day_log = last_train + np.cumsum(future_day_log_diff)\n",
    "    future_day = np.exp(future_day_log)\n",
    "    print('For crypto:',c,'today was',df['Close'].iloc[-1],'and tomorrow will be :',future_day[Ty-1])\n",
    "    print('So difference for:',c,'is:',future_day[Ty-1] - df['Close'].iloc[-1],'which is per dollar:',round((future_day[Ty-1] - df['Close'].iloc[-1])/future_day[Ty-1],5))"
   ],
   "id": "246536775c407f41",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 - 2s - 37ms/step - loss: 837.7582\n",
      "Epoch 2/100\n",
      "45/45 - 0s - 2ms/step - loss: 770.6459\n",
      "Epoch 3/100\n",
      "45/45 - 0s - 2ms/step - loss: 662.1416\n",
      "Epoch 4/100\n",
      "45/45 - 0s - 2ms/step - loss: 649.8273\n",
      "Epoch 5/100\n",
      "45/45 - 0s - 2ms/step - loss: 599.6901\n",
      "Epoch 6/100\n",
      "45/45 - 0s - 2ms/step - loss: 522.7653\n",
      "Epoch 7/100\n",
      "45/45 - 0s - 2ms/step - loss: 563.0628\n",
      "Epoch 8/100\n",
      "45/45 - 0s - 2ms/step - loss: 503.8659\n",
      "Epoch 9/100\n",
      "45/45 - 0s - 2ms/step - loss: 462.2401\n",
      "Epoch 10/100\n",
      "45/45 - 0s - 2ms/step - loss: 439.0882\n",
      "Epoch 11/100\n",
      "45/45 - 0s - 2ms/step - loss: 447.3350\n",
      "Epoch 12/100\n",
      "45/45 - 0s - 2ms/step - loss: 410.9137\n",
      "Epoch 13/100\n",
      "45/45 - 0s - 2ms/step - loss: 415.4202\n",
      "Epoch 14/100\n",
      "45/45 - 0s - 2ms/step - loss: 385.6503\n",
      "Epoch 15/100\n",
      "45/45 - 0s - 2ms/step - loss: 362.3763\n",
      "Epoch 16/100\n",
      "45/45 - 0s - 2ms/step - loss: 329.0608\n",
      "Epoch 17/100\n",
      "45/45 - 0s - 2ms/step - loss: 351.4658\n",
      "Epoch 18/100\n",
      "45/45 - 0s - 2ms/step - loss: 314.6415\n",
      "Epoch 19/100\n",
      "45/45 - 0s - 2ms/step - loss: 322.1246\n",
      "Epoch 20/100\n",
      "45/45 - 0s - 2ms/step - loss: 293.2323\n",
      "Epoch 21/100\n",
      "45/45 - 0s - 2ms/step - loss: 269.6295\n",
      "Epoch 22/100\n",
      "45/45 - 0s - 2ms/step - loss: 283.9032\n",
      "Epoch 23/100\n",
      "45/45 - 0s - 3ms/step - loss: 238.9055\n",
      "Epoch 24/100\n",
      "45/45 - 0s - 2ms/step - loss: 235.8581\n",
      "Epoch 25/100\n",
      "45/45 - 0s - 2ms/step - loss: 243.4504\n",
      "Epoch 26/100\n",
      "45/45 - 0s - 2ms/step - loss: 220.3099\n",
      "Epoch 27/100\n",
      "45/45 - 0s - 2ms/step - loss: 220.7740\n",
      "Epoch 28/100\n",
      "45/45 - 0s - 2ms/step - loss: 225.4342\n",
      "Epoch 29/100\n",
      "45/45 - 0s - 3ms/step - loss: 197.2316\n",
      "Epoch 30/100\n",
      "45/45 - 0s - 2ms/step - loss: 195.6129\n",
      "Epoch 31/100\n",
      "45/45 - 0s - 2ms/step - loss: 191.1464\n",
      "Epoch 32/100\n",
      "45/45 - 0s - 2ms/step - loss: 190.8638\n",
      "Epoch 33/100\n",
      "45/45 - 0s - 3ms/step - loss: 171.1947\n",
      "Epoch 34/100\n",
      "45/45 - 0s - 3ms/step - loss: 174.0046\n",
      "Epoch 35/100\n",
      "45/45 - 0s - 2ms/step - loss: 177.3900\n",
      "Epoch 36/100\n",
      "45/45 - 0s - 2ms/step - loss: 159.8783\n",
      "Epoch 37/100\n",
      "45/45 - 0s - 2ms/step - loss: 160.6521\n",
      "Epoch 38/100\n",
      "45/45 - 0s - 3ms/step - loss: 157.1927\n",
      "Epoch 39/100\n",
      "45/45 - 0s - 3ms/step - loss: 154.2878\n",
      "Epoch 40/100\n",
      "45/45 - 0s - 2ms/step - loss: 150.4971\n",
      "Epoch 41/100\n",
      "45/45 - 0s - 2ms/step - loss: 152.6858\n",
      "Epoch 42/100\n",
      "45/45 - 0s - 2ms/step - loss: 149.8879\n",
      "Epoch 43/100\n",
      "45/45 - 0s - 2ms/step - loss: 143.6635\n",
      "Epoch 44/100\n",
      "45/45 - 0s - 2ms/step - loss: 143.0394\n",
      "Epoch 45/100\n",
      "45/45 - 0s - 2ms/step - loss: 140.6248\n",
      "Epoch 46/100\n",
      "45/45 - 0s - 2ms/step - loss: 139.8577\n",
      "Epoch 47/100\n",
      "45/45 - 0s - 2ms/step - loss: 137.4824\n",
      "Epoch 48/100\n",
      "45/45 - 0s - 2ms/step - loss: 135.3669\n",
      "Epoch 49/100\n",
      "45/45 - 0s - 3ms/step - loss: 135.8965\n",
      "Epoch 50/100\n",
      "45/45 - 0s - 2ms/step - loss: 132.6436\n",
      "Epoch 51/100\n",
      "45/45 - 0s - 3ms/step - loss: 132.3361\n",
      "Epoch 52/100\n",
      "45/45 - 0s - 2ms/step - loss: 129.1722\n",
      "Epoch 53/100\n",
      "45/45 - 0s - 3ms/step - loss: 129.5432\n",
      "Epoch 54/100\n",
      "45/45 - 0s - 3ms/step - loss: 127.3613\n",
      "Epoch 55/100\n",
      "45/45 - 0s - 3ms/step - loss: 128.5793\n",
      "Epoch 56/100\n",
      "45/45 - 0s - 2ms/step - loss: 123.7141\n",
      "Epoch 57/100\n",
      "45/45 - 0s - 2ms/step - loss: 123.6163\n",
      "Epoch 58/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.5784\n",
      "Epoch 59/100\n",
      "45/45 - 0s - 2ms/step - loss: 123.1526\n",
      "Epoch 60/100\n",
      "45/45 - 0s - 2ms/step - loss: 120.8714\n",
      "Epoch 61/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.3482\n",
      "Epoch 62/100\n",
      "45/45 - 0s - 2ms/step - loss: 120.0571\n",
      "Epoch 63/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.8242\n",
      "Epoch 64/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.5149\n",
      "Epoch 65/100\n",
      "45/45 - 0s - 2ms/step - loss: 117.9637\n",
      "Epoch 66/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.1879\n",
      "Epoch 67/100\n",
      "45/45 - 0s - 2ms/step - loss: 117.1500\n",
      "Epoch 68/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.0462\n",
      "Epoch 69/100\n",
      "45/45 - 0s - 2ms/step - loss: 112.8627\n",
      "Epoch 70/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.9179\n",
      "Epoch 71/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.2029\n",
      "Epoch 72/100\n",
      "45/45 - 0s - 2ms/step - loss: 114.7047\n",
      "Epoch 73/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.4380\n",
      "Epoch 74/100\n",
      "45/45 - 0s - 2ms/step - loss: 112.6938\n",
      "Epoch 75/100\n",
      "45/45 - 0s - 2ms/step - loss: 112.3860\n",
      "Epoch 76/100\n",
      "45/45 - 0s - 2ms/step - loss: 113.0392\n",
      "Epoch 77/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.3471\n",
      "Epoch 78/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.5722\n",
      "Epoch 79/100\n",
      "45/45 - 0s - 2ms/step - loss: 112.9299\n",
      "Epoch 80/100\n",
      "45/45 - 0s - 2ms/step - loss: 110.6812\n",
      "Epoch 81/100\n",
      "45/45 - 0s - 2ms/step - loss: 110.5939\n",
      "Epoch 82/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.5710\n",
      "Epoch 83/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.5132\n",
      "Epoch 84/100\n",
      "45/45 - 0s - 2ms/step - loss: 109.7893\n",
      "Epoch 85/100\n",
      "45/45 - 0s - 2ms/step - loss: 110.7408\n",
      "Epoch 86/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.8430\n",
      "Epoch 87/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.0350\n",
      "Epoch 88/100\n",
      "45/45 - 0s - 2ms/step - loss: 109.7719\n",
      "Epoch 89/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.8711\n",
      "Epoch 90/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.0988\n",
      "Epoch 91/100\n",
      "45/45 - 0s - 2ms/step - loss: 109.8022\n",
      "Epoch 92/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.1133\n",
      "Epoch 93/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.4442\n",
      "Epoch 94/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.7452\n",
      "Epoch 95/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.9112\n",
      "Epoch 96/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.7490\n",
      "Epoch 97/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.5682\n",
      "Epoch 98/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.9737\n",
      "Epoch 99/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.5015\n",
      "Epoch 100/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.1656\n",
      "1/1 - 0s - 86ms/step\n",
      "For crypto: BTC-USD today was 65801.7265625 and tomorrow will be : 68031.664\n",
      "So difference for: BTC-USD is: 2229.9375 which is per dollar: 0.03278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 - 2s - 35ms/step - loss: 811.2491\n",
      "Epoch 2/100\n",
      "45/45 - 0s - 2ms/step - loss: 674.1835\n",
      "Epoch 3/100\n",
      "45/45 - 0s - 2ms/step - loss: 567.9548\n",
      "Epoch 4/100\n",
      "45/45 - 0s - 2ms/step - loss: 527.2013\n",
      "Epoch 5/100\n",
      "45/45 - 0s - 2ms/step - loss: 492.8159\n",
      "Epoch 6/100\n",
      "45/45 - 0s - 2ms/step - loss: 448.7288\n",
      "Epoch 7/100\n",
      "45/45 - 0s - 2ms/step - loss: 422.1017\n",
      "Epoch 8/100\n",
      "45/45 - 0s - 2ms/step - loss: 401.5456\n",
      "Epoch 9/100\n",
      "45/45 - 0s - 2ms/step - loss: 415.0064\n",
      "Epoch 10/100\n",
      "45/45 - 0s - 2ms/step - loss: 397.8583\n",
      "Epoch 11/100\n",
      "45/45 - 0s - 2ms/step - loss: 366.6409\n",
      "Epoch 12/100\n",
      "45/45 - 0s - 3ms/step - loss: 349.5034\n",
      "Epoch 13/100\n",
      "45/45 - 0s - 2ms/step - loss: 345.1366\n",
      "Epoch 14/100\n",
      "45/45 - 0s - 2ms/step - loss: 329.1364\n",
      "Epoch 15/100\n",
      "45/45 - 0s - 2ms/step - loss: 310.2889\n",
      "Epoch 16/100\n",
      "45/45 - 0s - 3ms/step - loss: 323.1534\n",
      "Epoch 17/100\n",
      "45/45 - 0s - 2ms/step - loss: 301.7808\n",
      "Epoch 18/100\n",
      "45/45 - 0s - 2ms/step - loss: 299.3804\n",
      "Epoch 19/100\n",
      "45/45 - 0s - 2ms/step - loss: 278.7061\n",
      "Epoch 20/100\n",
      "45/45 - 0s - 2ms/step - loss: 287.0596\n",
      "Epoch 21/100\n",
      "45/45 - 0s - 2ms/step - loss: 267.8541\n",
      "Epoch 22/100\n",
      "45/45 - 0s - 2ms/step - loss: 257.1992\n",
      "Epoch 23/100\n",
      "45/45 - 0s - 2ms/step - loss: 248.8177\n",
      "Epoch 24/100\n",
      "45/45 - 0s - 2ms/step - loss: 240.0822\n",
      "Epoch 25/100\n",
      "45/45 - 0s - 2ms/step - loss: 239.8195\n",
      "Epoch 26/100\n",
      "45/45 - 0s - 2ms/step - loss: 229.3542\n",
      "Epoch 27/100\n",
      "45/45 - 0s - 2ms/step - loss: 224.2418\n",
      "Epoch 28/100\n",
      "45/45 - 0s - 2ms/step - loss: 216.1511\n",
      "Epoch 29/100\n",
      "45/45 - 0s - 2ms/step - loss: 212.4140\n",
      "Epoch 30/100\n",
      "45/45 - 0s - 2ms/step - loss: 212.4623\n",
      "Epoch 31/100\n",
      "45/45 - 0s - 2ms/step - loss: 200.6194\n",
      "Epoch 32/100\n",
      "45/45 - 0s - 2ms/step - loss: 195.3047\n",
      "Epoch 33/100\n",
      "45/45 - 0s - 2ms/step - loss: 186.0388\n",
      "Epoch 34/100\n",
      "45/45 - 0s - 2ms/step - loss: 183.1378\n",
      "Epoch 35/100\n",
      "45/45 - 0s - 2ms/step - loss: 176.2088\n",
      "Epoch 36/100\n",
      "45/45 - 0s - 2ms/step - loss: 175.0481\n",
      "Epoch 37/100\n",
      "45/45 - 0s - 2ms/step - loss: 166.7956\n",
      "Epoch 38/100\n",
      "45/45 - 0s - 2ms/step - loss: 165.4737\n",
      "Epoch 39/100\n",
      "45/45 - 0s - 2ms/step - loss: 157.6436\n",
      "Epoch 40/100\n",
      "45/45 - 0s - 2ms/step - loss: 159.0434\n",
      "Epoch 41/100\n",
      "45/45 - 0s - 2ms/step - loss: 152.7137\n",
      "Epoch 42/100\n",
      "45/45 - 0s - 2ms/step - loss: 153.3782\n",
      "Epoch 43/100\n",
      "45/45 - 0s - 2ms/step - loss: 147.6472\n",
      "Epoch 44/100\n",
      "45/45 - 0s - 2ms/step - loss: 142.6709\n",
      "Epoch 45/100\n",
      "45/45 - 0s - 2ms/step - loss: 144.1000\n",
      "Epoch 46/100\n",
      "45/45 - 0s - 2ms/step - loss: 140.5177\n",
      "Epoch 47/100\n",
      "45/45 - 0s - 2ms/step - loss: 137.8178\n",
      "Epoch 48/100\n",
      "45/45 - 0s - 2ms/step - loss: 133.6041\n",
      "Epoch 49/100\n",
      "45/45 - 0s - 2ms/step - loss: 134.2205\n",
      "Epoch 50/100\n",
      "45/45 - 0s - 2ms/step - loss: 129.2453\n",
      "Epoch 51/100\n",
      "45/45 - 0s - 2ms/step - loss: 129.8683\n",
      "Epoch 52/100\n",
      "45/45 - 0s - 2ms/step - loss: 128.9699\n",
      "Epoch 53/100\n",
      "45/45 - 0s - 2ms/step - loss: 126.3918\n",
      "Epoch 54/100\n",
      "45/45 - 0s - 2ms/step - loss: 124.7088\n",
      "Epoch 55/100\n",
      "45/45 - 0s - 2ms/step - loss: 122.3859\n",
      "Epoch 56/100\n",
      "45/45 - 0s - 2ms/step - loss: 122.2115\n",
      "Epoch 57/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.2158\n",
      "Epoch 58/100\n",
      "45/45 - 0s - 2ms/step - loss: 117.8485\n",
      "Epoch 59/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.7480\n",
      "Epoch 60/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.5241\n",
      "Epoch 61/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.9661\n",
      "Epoch 62/100\n",
      "45/45 - 0s - 2ms/step - loss: 114.7665\n",
      "Epoch 63/100\n",
      "45/45 - 0s - 2ms/step - loss: 114.3543\n",
      "Epoch 64/100\n",
      "45/45 - 0s - 2ms/step - loss: 114.4587\n",
      "Epoch 65/100\n",
      "45/45 - 0s - 2ms/step - loss: 113.0790\n",
      "Epoch 66/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.6234\n",
      "Epoch 67/100\n",
      "45/45 - 0s - 2ms/step - loss: 111.1687\n",
      "Epoch 68/100\n",
      "45/45 - 0s - 2ms/step - loss: 112.5943\n",
      "Epoch 69/100\n",
      "45/45 - 0s - 2ms/step - loss: 110.1410\n",
      "Epoch 70/100\n",
      "45/45 - 0s - 2ms/step - loss: 109.0726\n",
      "Epoch 71/100\n",
      "45/45 - 0s - 2ms/step - loss: 110.0690\n",
      "Epoch 72/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.8532\n",
      "Epoch 73/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.7247\n",
      "Epoch 74/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.6253\n",
      "Epoch 75/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.8291\n",
      "Epoch 76/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.2499\n",
      "Epoch 77/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.9445\n",
      "Epoch 78/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.4040\n",
      "Epoch 79/100\n",
      "45/45 - 0s - 2ms/step - loss: 108.2042\n",
      "Epoch 80/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.0104\n",
      "Epoch 81/100\n",
      "45/45 - 0s - 2ms/step - loss: 107.3400\n",
      "Epoch 82/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.8726\n",
      "Epoch 83/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.0368\n",
      "Epoch 84/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.5380\n",
      "Epoch 85/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.5755\n",
      "Epoch 86/100\n",
      "45/45 - 0s - 2ms/step - loss: 106.5201\n",
      "Epoch 87/100\n",
      "45/45 - 0s - 2ms/step - loss: 104.9874\n",
      "Epoch 88/100\n",
      "45/45 - 0s - 2ms/step - loss: 104.8947\n",
      "Epoch 89/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.4251\n",
      "Epoch 90/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.4971\n",
      "Epoch 91/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.0183\n",
      "Epoch 92/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.4561\n",
      "Epoch 93/100\n",
      "45/45 - 0s - 2ms/step - loss: 104.2905\n",
      "Epoch 94/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.0295\n",
      "Epoch 95/100\n",
      "45/45 - 0s - 2ms/step - loss: 103.9766\n",
      "Epoch 96/100\n",
      "45/45 - 0s - 2ms/step - loss: 105.2524\n",
      "Epoch 97/100\n",
      "45/45 - 0s - 2ms/step - loss: 103.9603\n",
      "Epoch 98/100\n",
      "45/45 - 0s - 2ms/step - loss: 104.5607\n",
      "Epoch 99/100\n",
      "45/45 - 0s - 2ms/step - loss: 103.8337\n",
      "Epoch 100/100\n",
      "45/45 - 0s - 2ms/step - loss: 104.8667\n",
      "1/1 - 0s - 95ms/step\n",
      "For crypto: BNB-USD today was 598.4732666015625 and tomorrow will be : 611.9259\n",
      "So difference for: BNB-USD is: 13.45263671875 which is per dollar: 0.02198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 - 3s - 70ms/step - loss: 745.1126\n",
      "Epoch 2/100\n",
      "45/45 - 0s - 2ms/step - loss: 712.7032\n",
      "Epoch 3/100\n",
      "45/45 - 0s - 2ms/step - loss: 595.4241\n",
      "Epoch 4/100\n",
      "45/45 - 0s - 2ms/step - loss: 547.5791\n",
      "Epoch 5/100\n",
      "45/45 - 0s - 2ms/step - loss: 520.5189\n",
      "Epoch 6/100\n",
      "45/45 - 0s - 2ms/step - loss: 537.1536\n",
      "Epoch 7/100\n",
      "45/45 - 0s - 2ms/step - loss: 478.2292\n",
      "Epoch 8/100\n",
      "45/45 - 0s - 2ms/step - loss: 464.1445\n",
      "Epoch 9/100\n",
      "45/45 - 0s - 2ms/step - loss: 424.7892\n",
      "Epoch 10/100\n",
      "45/45 - 0s - 2ms/step - loss: 415.3658\n",
      "Epoch 11/100\n",
      "45/45 - 0s - 2ms/step - loss: 395.4176\n",
      "Epoch 12/100\n",
      "45/45 - 0s - 2ms/step - loss: 381.2562\n",
      "Epoch 13/100\n",
      "45/45 - 0s - 2ms/step - loss: 357.4089\n",
      "Epoch 14/100\n",
      "45/45 - 0s - 2ms/step - loss: 354.3046\n",
      "Epoch 15/100\n",
      "45/45 - 0s - 2ms/step - loss: 354.6545\n",
      "Epoch 16/100\n",
      "45/45 - 0s - 2ms/step - loss: 338.3276\n",
      "Epoch 17/100\n",
      "45/45 - 0s - 2ms/step - loss: 330.7523\n",
      "Epoch 18/100\n",
      "45/45 - 0s - 2ms/step - loss: 326.5618\n",
      "Epoch 19/100\n",
      "45/45 - 0s - 2ms/step - loss: 313.6386\n",
      "Epoch 20/100\n",
      "45/45 - 0s - 2ms/step - loss: 314.2839\n",
      "Epoch 21/100\n",
      "45/45 - 0s - 2ms/step - loss: 294.0838\n",
      "Epoch 22/100\n",
      "45/45 - 0s - 2ms/step - loss: 287.6042\n",
      "Epoch 23/100\n",
      "45/45 - 0s - 2ms/step - loss: 263.6879\n",
      "Epoch 24/100\n",
      "45/45 - 0s - 2ms/step - loss: 295.2616\n",
      "Epoch 25/100\n",
      "45/45 - 0s - 2ms/step - loss: 260.2898\n",
      "Epoch 26/100\n",
      "45/45 - 0s - 2ms/step - loss: 265.5175\n",
      "Epoch 27/100\n",
      "45/45 - 0s - 2ms/step - loss: 257.7781\n",
      "Epoch 28/100\n",
      "45/45 - 0s - 2ms/step - loss: 259.3773\n",
      "Epoch 29/100\n",
      "45/45 - 0s - 2ms/step - loss: 240.0556\n",
      "Epoch 30/100\n",
      "45/45 - 0s - 2ms/step - loss: 238.2992\n",
      "Epoch 31/100\n",
      "45/45 - 0s - 2ms/step - loss: 246.8564\n",
      "Epoch 32/100\n",
      "45/45 - 0s - 2ms/step - loss: 222.3779\n",
      "Epoch 33/100\n",
      "45/45 - 0s - 2ms/step - loss: 230.7319\n",
      "Epoch 34/100\n",
      "45/45 - 0s - 2ms/step - loss: 217.4993\n",
      "Epoch 35/100\n",
      "45/45 - 0s - 2ms/step - loss: 224.4652\n",
      "Epoch 36/100\n",
      "45/45 - 0s - 2ms/step - loss: 211.9614\n",
      "Epoch 37/100\n",
      "45/45 - 0s - 2ms/step - loss: 215.7225\n",
      "Epoch 38/100\n",
      "45/45 - 0s - 2ms/step - loss: 210.7205\n",
      "Epoch 39/100\n",
      "45/45 - 0s - 2ms/step - loss: 206.2390\n",
      "Epoch 40/100\n",
      "45/45 - 0s - 2ms/step - loss: 191.7390\n",
      "Epoch 41/100\n",
      "45/45 - 0s - 2ms/step - loss: 191.9099\n",
      "Epoch 42/100\n",
      "45/45 - 0s - 2ms/step - loss: 186.4483\n",
      "Epoch 43/100\n",
      "45/45 - 0s - 2ms/step - loss: 188.1461\n",
      "Epoch 44/100\n",
      "45/45 - 0s - 2ms/step - loss: 184.6345\n",
      "Epoch 45/100\n",
      "45/45 - 0s - 2ms/step - loss: 175.2378\n",
      "Epoch 46/100\n",
      "45/45 - 0s - 2ms/step - loss: 182.1973\n",
      "Epoch 47/100\n",
      "45/45 - 0s - 2ms/step - loss: 177.0288\n",
      "Epoch 48/100\n",
      "45/45 - 0s - 2ms/step - loss: 175.8310\n",
      "Epoch 49/100\n",
      "45/45 - 0s - 2ms/step - loss: 168.2842\n",
      "Epoch 50/100\n",
      "45/45 - 0s - 2ms/step - loss: 167.1422\n",
      "Epoch 51/100\n",
      "45/45 - 0s - 2ms/step - loss: 164.5739\n",
      "Epoch 52/100\n",
      "45/45 - 0s - 2ms/step - loss: 162.9429\n",
      "Epoch 53/100\n",
      "45/45 - 0s - 2ms/step - loss: 164.3574\n",
      "Epoch 54/100\n",
      "45/45 - 0s - 2ms/step - loss: 155.2183\n",
      "Epoch 55/100\n",
      "45/45 - 0s - 2ms/step - loss: 161.1297\n",
      "Epoch 56/100\n",
      "45/45 - 0s - 2ms/step - loss: 157.1762\n",
      "Epoch 57/100\n",
      "45/45 - 0s - 2ms/step - loss: 152.7498\n",
      "Epoch 58/100\n",
      "45/45 - 0s - 2ms/step - loss: 144.8430\n",
      "Epoch 59/100\n",
      "45/45 - 0s - 2ms/step - loss: 148.8628\n",
      "Epoch 60/100\n",
      "45/45 - 0s - 2ms/step - loss: 149.1828\n",
      "Epoch 61/100\n",
      "45/45 - 0s - 2ms/step - loss: 147.1852\n",
      "Epoch 62/100\n",
      "45/45 - 0s - 2ms/step - loss: 148.5493\n",
      "Epoch 63/100\n",
      "45/45 - 0s - 2ms/step - loss: 137.7195\n",
      "Epoch 64/100\n",
      "45/45 - 0s - 2ms/step - loss: 140.3958\n",
      "Epoch 65/100\n",
      "45/45 - 0s - 2ms/step - loss: 139.9364\n",
      "Epoch 66/100\n",
      "45/45 - 0s - 2ms/step - loss: 138.2589\n",
      "Epoch 67/100\n",
      "45/45 - 0s - 2ms/step - loss: 140.3652\n",
      "Epoch 68/100\n",
      "45/45 - 0s - 2ms/step - loss: 135.2126\n",
      "Epoch 69/100\n",
      "45/45 - 0s - 2ms/step - loss: 134.7924\n",
      "Epoch 70/100\n",
      "45/45 - 0s - 2ms/step - loss: 133.8157\n",
      "Epoch 71/100\n",
      "45/45 - 0s - 2ms/step - loss: 133.7723\n",
      "Epoch 72/100\n",
      "45/45 - 0s - 2ms/step - loss: 130.6684\n",
      "Epoch 73/100\n",
      "45/45 - 0s - 2ms/step - loss: 130.0577\n",
      "Epoch 74/100\n",
      "45/45 - 0s - 2ms/step - loss: 128.4114\n",
      "Epoch 75/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.8334\n",
      "Epoch 76/100\n",
      "45/45 - 0s - 2ms/step - loss: 128.8861\n",
      "Epoch 77/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.7867\n",
      "Epoch 78/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.2809\n",
      "Epoch 79/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.2928\n",
      "Epoch 80/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.7845\n",
      "Epoch 81/100\n",
      "45/45 - 0s - 2ms/step - loss: 124.2515\n",
      "Epoch 82/100\n",
      "45/45 - 0s - 2ms/step - loss: 122.2398\n",
      "Epoch 83/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.9047\n",
      "Epoch 84/100\n",
      "45/45 - 0s - 2ms/step - loss: 123.2350\n",
      "Epoch 85/100\n",
      "45/45 - 0s - 2ms/step - loss: 122.0815\n",
      "Epoch 86/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.3645\n",
      "Epoch 87/100\n",
      "45/45 - 0s - 2ms/step - loss: 119.7695\n",
      "Epoch 88/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.2488\n",
      "Epoch 89/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.7343\n",
      "Epoch 90/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.4248\n",
      "Epoch 91/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.6838\n",
      "Epoch 92/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.8704\n",
      "Epoch 93/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.1104\n",
      "Epoch 94/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.2069\n",
      "Epoch 95/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.1659\n",
      "Epoch 96/100\n",
      "45/45 - 0s - 2ms/step - loss: 116.6165\n",
      "Epoch 97/100\n",
      "45/45 - 0s - 2ms/step - loss: 118.2315\n",
      "Epoch 98/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.2095\n",
      "Epoch 99/100\n",
      "45/45 - 0s - 2ms/step - loss: 114.2052\n",
      "Epoch 100/100\n",
      "45/45 - 0s - 2ms/step - loss: 115.1800\n",
      "1/1 - 0s - 80ms/step\n",
      "For crypto: ETH-USD today was 2664.958251953125 and tomorrow will be : 2714.5051\n",
      "So difference for: ETH-USD is: 49.546875 which is per dollar: 0.01825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 144 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 - 2s - 55ms/step - loss: 747.8218\n",
      "Epoch 2/100\n",
      "45/45 - 0s - 3ms/step - loss: 652.5847\n",
      "Epoch 3/100\n",
      "45/45 - 0s - 2ms/step - loss: 594.6823\n",
      "Epoch 4/100\n",
      "45/45 - 0s - 2ms/step - loss: 535.3329\n",
      "Epoch 5/100\n",
      "45/45 - 0s - 2ms/step - loss: 506.5260\n",
      "Epoch 6/100\n",
      "45/45 - 0s - 2ms/step - loss: 445.1516\n",
      "Epoch 7/100\n",
      "45/45 - 0s - 3ms/step - loss: 447.8998\n",
      "Epoch 8/100\n",
      "45/45 - 0s - 3ms/step - loss: 415.8036\n",
      "Epoch 9/100\n",
      "45/45 - 0s - 2ms/step - loss: 422.8457\n",
      "Epoch 10/100\n",
      "45/45 - 0s - 2ms/step - loss: 396.9376\n",
      "Epoch 11/100\n",
      "45/45 - 0s - 2ms/step - loss: 395.2333\n",
      "Epoch 12/100\n",
      "45/45 - 0s - 3ms/step - loss: 364.6440\n",
      "Epoch 13/100\n",
      "45/45 - 0s - 2ms/step - loss: 349.6243\n",
      "Epoch 14/100\n",
      "45/45 - 0s - 3ms/step - loss: 373.7072\n",
      "Epoch 15/100\n",
      "45/45 - 0s - 2ms/step - loss: 338.3277\n",
      "Epoch 16/100\n",
      "45/45 - 0s - 2ms/step - loss: 335.3015\n",
      "Epoch 17/100\n",
      "45/45 - 0s - 2ms/step - loss: 325.7258\n",
      "Epoch 18/100\n",
      "45/45 - 0s - 2ms/step - loss: 309.8746\n",
      "Epoch 19/100\n",
      "45/45 - 0s - 2ms/step - loss: 297.9003\n",
      "Epoch 20/100\n",
      "45/45 - 0s - 2ms/step - loss: 316.0318\n",
      "Epoch 21/100\n",
      "45/45 - 0s - 3ms/step - loss: 297.5783\n",
      "Epoch 22/100\n",
      "45/45 - 0s - 3ms/step - loss: 271.3645\n",
      "Epoch 23/100\n",
      "45/45 - 0s - 2ms/step - loss: 285.4486\n",
      "Epoch 24/100\n",
      "45/45 - 0s - 2ms/step - loss: 275.0340\n",
      "Epoch 25/100\n",
      "45/45 - 0s - 2ms/step - loss: 259.5476\n",
      "Epoch 26/100\n",
      "45/45 - 0s - 3ms/step - loss: 273.2388\n",
      "Epoch 27/100\n",
      "45/45 - 0s - 2ms/step - loss: 256.3097\n",
      "Epoch 28/100\n",
      "45/45 - 0s - 3ms/step - loss: 253.5440\n",
      "Epoch 29/100\n",
      "45/45 - 0s - 2ms/step - loss: 256.6451\n",
      "Epoch 30/100\n",
      "45/45 - 0s - 3ms/step - loss: 245.4992\n",
      "Epoch 31/100\n",
      "45/45 - 0s - 2ms/step - loss: 247.5356\n",
      "Epoch 32/100\n",
      "45/45 - 0s - 2ms/step - loss: 234.4659\n",
      "Epoch 33/100\n",
      "45/45 - 0s - 2ms/step - loss: 220.6123\n",
      "Epoch 34/100\n",
      "45/45 - 0s - 2ms/step - loss: 227.6408\n",
      "Epoch 35/100\n",
      "45/45 - 0s - 2ms/step - loss: 224.6213\n",
      "Epoch 36/100\n",
      "45/45 - 0s - 2ms/step - loss: 238.4011\n",
      "Epoch 37/100\n",
      "45/45 - 0s - 2ms/step - loss: 213.3463\n",
      "Epoch 38/100\n",
      "45/45 - 0s - 2ms/step - loss: 210.6575\n",
      "Epoch 39/100\n",
      "45/45 - 0s - 2ms/step - loss: 202.7415\n",
      "Epoch 40/100\n",
      "45/45 - 0s - 3ms/step - loss: 203.7799\n",
      "Epoch 41/100\n",
      "45/45 - 0s - 2ms/step - loss: 198.3877\n",
      "Epoch 42/100\n",
      "45/45 - 0s - 2ms/step - loss: 200.7952\n",
      "Epoch 43/100\n",
      "45/45 - 0s - 2ms/step - loss: 193.5521\n",
      "Epoch 44/100\n",
      "45/45 - 0s - 2ms/step - loss: 187.5602\n",
      "Epoch 45/100\n",
      "45/45 - 0s - 2ms/step - loss: 184.8515\n",
      "Epoch 46/100\n",
      "45/45 - 0s - 2ms/step - loss: 192.2897\n",
      "Epoch 47/100\n",
      "45/45 - 0s - 3ms/step - loss: 184.5053\n",
      "Epoch 48/100\n",
      "45/45 - 0s - 2ms/step - loss: 177.0346\n",
      "Epoch 49/100\n",
      "45/45 - 0s - 2ms/step - loss: 172.0692\n",
      "Epoch 50/100\n",
      "45/45 - 0s - 2ms/step - loss: 173.5585\n",
      "Epoch 51/100\n",
      "45/45 - 0s - 2ms/step - loss: 169.1748\n",
      "Epoch 52/100\n",
      "45/45 - 0s - 3ms/step - loss: 173.3988\n",
      "Epoch 53/100\n",
      "45/45 - 0s - 3ms/step - loss: 170.9644\n",
      "Epoch 54/100\n",
      "45/45 - 0s - 2ms/step - loss: 167.1829\n",
      "Epoch 55/100\n",
      "45/45 - 0s - 2ms/step - loss: 157.5150\n",
      "Epoch 56/100\n",
      "45/45 - 0s - 3ms/step - loss: 161.0017\n",
      "Epoch 57/100\n",
      "45/45 - 0s - 2ms/step - loss: 164.6260\n",
      "Epoch 58/100\n",
      "45/45 - 0s - 3ms/step - loss: 154.8524\n",
      "Epoch 59/100\n",
      "45/45 - 0s - 2ms/step - loss: 155.4303\n",
      "Epoch 60/100\n",
      "45/45 - 0s - 2ms/step - loss: 156.3036\n",
      "Epoch 61/100\n",
      "45/45 - 0s - 2ms/step - loss: 151.5527\n",
      "Epoch 62/100\n",
      "45/45 - 0s - 3ms/step - loss: 145.9160\n",
      "Epoch 63/100\n",
      "45/45 - 0s - 3ms/step - loss: 139.9633\n",
      "Epoch 64/100\n",
      "45/45 - 0s - 2ms/step - loss: 147.2427\n",
      "Epoch 65/100\n",
      "45/45 - 0s - 2ms/step - loss: 139.1863\n",
      "Epoch 66/100\n",
      "45/45 - 0s - 2ms/step - loss: 140.3305\n",
      "Epoch 67/100\n",
      "45/45 - 0s - 2ms/step - loss: 144.0252\n",
      "Epoch 68/100\n",
      "45/45 - 0s - 3ms/step - loss: 139.4874\n",
      "Epoch 69/100\n",
      "45/45 - 0s - 2ms/step - loss: 139.1572\n",
      "Epoch 70/100\n",
      "45/45 - 0s - 2ms/step - loss: 137.9720\n",
      "Epoch 71/100\n",
      "45/45 - 0s - 3ms/step - loss: 137.1145\n",
      "Epoch 72/100\n",
      "45/45 - 0s - 2ms/step - loss: 134.6462\n",
      "Epoch 73/100\n",
      "45/45 - 0s - 3ms/step - loss: 134.9657\n",
      "Epoch 74/100\n",
      "45/45 - 0s - 2ms/step - loss: 132.3417\n",
      "Epoch 75/100\n",
      "45/45 - 0s - 2ms/step - loss: 131.6949\n",
      "Epoch 76/100\n",
      "45/45 - 0s - 2ms/step - loss: 131.0036\n",
      "Epoch 77/100\n",
      "45/45 - 0s - 2ms/step - loss: 127.5605\n",
      "Epoch 78/100\n",
      "45/45 - 0s - 3ms/step - loss: 129.2780\n",
      "Epoch 79/100\n",
      "45/45 - 0s - 2ms/step - loss: 132.1237\n",
      "Epoch 80/100\n",
      "45/45 - 0s - 2ms/step - loss: 126.4197\n",
      "Epoch 81/100\n",
      "45/45 - 0s - 3ms/step - loss: 125.4942\n",
      "Epoch 82/100\n",
      "45/45 - 0s - 3ms/step - loss: 124.0867\n",
      "Epoch 83/100\n",
      "45/45 - 0s - 2ms/step - loss: 126.3778\n",
      "Epoch 84/100\n",
      "45/45 - 0s - 2ms/step - loss: 124.7255\n",
      "Epoch 85/100\n",
      "45/45 - 0s - 3ms/step - loss: 124.9962\n",
      "Epoch 86/100\n",
      "45/45 - 0s - 2ms/step - loss: 125.0646\n",
      "Epoch 87/100\n",
      "45/45 - 0s - 3ms/step - loss: 122.2464\n",
      "Epoch 88/100\n",
      "45/45 - 0s - 3ms/step - loss: 122.8566\n",
      "Epoch 89/100\n",
      "45/45 - 0s - 3ms/step - loss: 121.5933\n",
      "Epoch 90/100\n",
      "45/45 - 0s - 3ms/step - loss: 120.9571\n",
      "Epoch 91/100\n",
      "45/45 - 0s - 3ms/step - loss: 120.6554\n",
      "Epoch 92/100\n",
      "45/45 - 0s - 2ms/step - loss: 122.5902\n",
      "Epoch 93/100\n",
      "45/45 - 0s - 2ms/step - loss: 121.1606\n",
      "Epoch 94/100\n",
      "45/45 - 0s - 3ms/step - loss: 119.0400\n",
      "Epoch 95/100\n",
      "45/45 - 0s - 2ms/step - loss: 119.9154\n",
      "Epoch 96/100\n",
      "45/45 - 0s - 3ms/step - loss: 120.0241\n",
      "Epoch 97/100\n",
      "45/45 - 0s - 3ms/step - loss: 117.5830\n",
      "Epoch 98/100\n",
      "45/45 - 0s - 2ms/step - loss: 119.0951\n",
      "Epoch 99/100\n",
      "45/45 - 0s - 3ms/step - loss: 119.5947\n",
      "Epoch 100/100\n",
      "45/45 - 0s - 2ms/step - loss: 119.9455\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002514B571A80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s - 110ms/step\n",
      "For crypto: SOL-USD today was 158.61782836914062 and tomorrow will be : 157.72198\n",
      "So difference for: SOL-USD is: -0.895843505859375 which is per dollar: -0.00568\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
